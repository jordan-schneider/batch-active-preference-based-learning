""" Runs the test set generated by post.py by generating fake reward weights and seeing how many
are caught by the preferences."""

import pickle
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import argh  # type: ignore
import numpy as np
from argh import arg
from numpy.linalg import norm
from scipy.stats import multivariate_normal  # type: ignore
from sklearn.metrics import confusion_matrix  # type: ignore

from post import filter_halfplanes

N_FEATURES = 4


def assert_normals(normals: np.ndarray) -> None:
    shape = normals.shape
    assert len(shape) == 2
    assert shape[1] == N_FEATURES


def normalize(vectors: np.ndarray) -> np.ndarray:
    """ Takes in a 2d array of row vectors and ensures each row vector has an L_2 norm of 1."""
    return (vectors.T / norm(vectors, axis=1)).T


def find_reward_boundary(
    normals: np.ndarray, n_rewards: int
) -> Tuple[np.ndarray, np.ndarray]:
    """ Generates n_rewards reward weight with L2 norm of one. """
    assert_normals(normals)
    assert n_rewards > 0

    dist = multivariate_normal(mean=np.zeros(N_FEATURES))

    rewards = normalize(dist.rvs(size=n_rewards))

    ground_truth_alignment = np.all(np.dot(rewards, normals.T) > 0, axis=1)

    assert ground_truth_alignment.shape == (n_rewards,)
    assert rewards.shape == (n_rewards, N_FEATURES)

    return rewards, ground_truth_alignment


def run_test(
    normals: np.ndarray, fake_rewards: np.ndarray, aligned: np.ndarray,
) -> np.ndarray:
    """Runs an alignment test on randomly generated fake reward weights. """
    assert_normals(normals)
    assert len(fake_rewards.shape) == 2
    assert fake_rewards.shape[0] == aligned.shape[0]

    for fake_reward in fake_rewards:
        assert np.abs(norm(fake_reward) - 1) < 0.0001

    if normals.shape[0] > 0:
        results = np.all(np.dot(fake_rewards, normals.T) > 0, axis=1)
        print(
            f"predicted true={np.sum(results)}, predicted false={results.shape[0] - np.sum(results)}"
        )
        return confusion_matrix(y_true=aligned, y_pred=results)
    else:
        return confusion_matrix(y_true=aligned, y_pred=np.ones(aligned.shape))


@arg("--epsilons", nargs="+", type=float)
@arg("--human-samples", nargs="+", type=int)
def run_tests(
    epsilons: List[float] = [0.0],
    n_rewards: int = 100,
    human_samples: List[int] = [1],
    n_model_samples: int = 1000,
    normals_name: Path = Path("normals.npy"),
    preferences_name: Path = Path("preferences.npy"),
    datadir: Path = Path("questions"),
    skip_remove_duplicates: bool = False,
    skip_noise_filtering: bool = False,
    skip_epsilon_filtering: bool = False,
    skip_redundancy_filtering: bool = False,
    replications: Optional[int] = None,
) -> None:
    """ Run tests with full data to determine how much reward noise gets"""
    if replications is not None:
        for replication in range(1, replications + 1):
            run_tests(
                epsilons,
                n_rewards,
                human_samples,
                n_model_samples,
                normals_name,
                preferences_name,
                datadir / str(replication),
                skip_remove_duplicates,
                skip_noise_filtering,
                skip_epsilon_filtering,
                skip_redundancy_filtering,
            )
        return

    normals = np.load(datadir / normals_name)
    preferences = np.load(datadir / preferences_name)

    assert preferences.shape[0] > 0

    normals = (normals.T * preferences).T

    assert_normals(normals)

    results: Dict[Tuple[float, int], np.ndarray] = dict()
    for epsilon in epsilons:
        filtered_normals, _ = filter_halfplanes(
            normals=normals,
            n_samples=n_model_samples,
            epsilon=epsilon,
            skip_noise_filtering=True,
            skip_remove_duplicates=True,
            skip_redundancy_filtering=True,
        )
        rewards, aligned = find_reward_boundary(filtered_normals, n_rewards)
        print(
            f"aligned={np.sum(aligned)}, unaligned={aligned.shape[0] - np.sum(aligned)}"
        )
        for n in human_samples:
            print(f"Working on epsilon={epsilon}, n={n}")
            filtered_normals = normals[:n]
            filtered_normals, _ = filter_halfplanes(
                normals=filtered_normals,
                n_samples=n_model_samples,
                epsilon=epsilon,
                skip_remove_duplicates=skip_remove_duplicates,
                skip_noise_filtering=skip_noise_filtering,
                skip_epsilon_filtering=skip_epsilon_filtering,
                skip_redundancy_filtering=skip_redundancy_filtering,
            )

            results[(epsilon, n)] = run_test(
                normals=filtered_normals, fake_rewards=rewards, aligned=aligned,
            )

    outname = "out"
    if skip_remove_duplicates:
        outname += ".skip_duplicates"
    if skip_noise_filtering:
        outname += ".skip_noise"
    if skip_epsilon_filtering:
        outname += ".skip_epsilon"
    if skip_redundancy_filtering:
        outname += ".skip_lp"
    outname += ".pkl"

    pickle.dump(results, open(datadir / outname, "wb"))


if __name__ == "__main__":
    argh.dispatch_command(run_tests)
